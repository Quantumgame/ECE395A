\subsection{Benchmarks and Goals}
Typical benchmarks by which hardware oriented neural nets are measured include power consumption, latency, throughput, cost, and accuracy of the model.  Indeed all of these will have to be considered as possible trade offs. ~\cite{DBLP:journals/corr/SzeCYE17} Making the neural net smaller will probably reduce the latency and increase the throughput at the cost of accuracy.  Adding more MACs will increase the power consumption while increasing throughput or decreasing latency.

Some trade-offs are simpler to consider than others.  For instance, if we have more MAC hardware, we can increase the throughput simply by placing them in parallel.  It is also possible that we place them in series (pipelining) which can reduce the critical path and thus decrease the latency.  Other trade-offs are more complicated.  For instance, reducing the complexity of the model may reduce the accuracy though it is not guaranteed to decrease the power significantly.  It turns out that convolutional layers tend to consume much more energy that fully connected layers ~\cite{Chen:2016:ESA:3001136.3001177} and thus even if we significantly prune the fully connected weights (which make up the vast majority of weights) the power consumption will be only marginally reduced.

In our project, we do not expect to beat Google's TPU or Microsoft's Catapult in terms of throughout, and values which are relevant to us like latency are not available since these are not the goals of large datacenters.  Therefore in order to measure the success of our project, we will focus on whether our hardware and neural network architectures are sufficient to achieve a reasonable and useful performance in modulation classification.  We will make use of the large amount of previous work done in hardware optimization and make any results and software we write freely available for future research.
