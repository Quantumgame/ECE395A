\subsection{Neural Net Optimization}
Deep neural networks were not designed with hardware efficiency in mind.  In order to tackle the difficult problem of both fitting a neural net on an FPGA and having it run fast, we should look at both optimization of the hardware itself and optimization of the neural net structure.  Existing approaches to optimizing neural networks for hardware fall into one of two categories: precision reduction and operation reduction. ~\cite{DBLP:journals/corr/SzeCYE17}  Precision reduction consists of reducing the number of bits used in computation, using fixed point arithmetic (GPUs use floating point), non-linear quantization, weight sharing, and Huffman coding. ~\cite{DBLP:journals/corr/HanMD15} Recent research has even taken this to extreme, constraining all weights to $\pm 1$. ~\cite{DBLP:journals/corr/CourbariauxB16}  Since custom hardware was not being seriously considered for neural networks until relatively recently, many of the algorithms for precision reduction are recently discovered.  However, operation reduction is capable of significantly reducing size and increasing speed even on GPUs and CPUs and therefore many powerfull ``pruning'' algorithms already exist such Optimal Brain Damage (OBD) ~\cite{Cun:1990:OBD:109230.109298} and Optimal Brain Surgery. ~\cite{Hassibi:1993:OBS:2987189.2987223}

\subsubsection{Quantization}
Some FPGAs have specialized hardware for fast multiplication.  In the case of Xilinx FPGAs, this fast multiplication hardware is only available for fixed point numbers, and a floating point implementation of a MAC would require large amount of hardware and would run slower.  Thus at a minimum requirement for functionality, the neural nets must be quantized to fixed point, or linear precision.  Obviously any amount of quantization introduces error, as an interesting aside, the signal to quantization error is given by roughly $6 dB$ per bit which can be derived from assuming that quantization error is uniformly distributed.  While it is fairly clear what this figure is in terms of quantizing the signal, it is not clear what exactly this implies for the quantization of filter coefficients.  When optimizing our structure, we are not limited to fixed schemes for weight quantization.  In particular, we know the exact weights we are using on every iteration and we can take advantage of that.  This has led to research being primarily focused on optimal weight quantization rather than activation (signal) quantization. ~\cite{DBLP:journals/corr/SzeCYE17}  

Before talking about advanced quantization techniques we give a brief review of typical number representations.  Fixed point representation is a class of number representations where the precision does not change.  That is, the number of digits before and after the binary point are constant.  These number are often represented in digital logic in the $Q$ format (Q is used since, roughly speaking, fixed point is to floating point as $\mathcal{Q}$ is to $\mathcal{R}$).  There are two parameters to a $Q$ representation, the number of digits before ($n$) and after ($m$) the binary point.  Precisely, the binary sequence $\{b_i\}_{i=0}^{n+m-1}$ has the associated value
$$ B = - b_0\times 2^{n-1} + \sum_{i=1}^{n+m-1} b_i \times 2^{n-i-1}$$

for example, assuming a two's complement representation, $Q1.7$ would have a maximum value of $\frac{127}{128}$, a minimum value of $-1$, and a precision of $\frac{1}{128}$.  The sequence $10010011$ would be associated with the value $-1 + \frac{1}{8} + \frac{1}{64} + \frac{1}{128} = -\frac{109}{128}$ 
An IEEE standard 32-bit floating point number is represented by 
$$ (-1)^s \times m \times 2^{(e-127)}$$
where $s$ is the sign bit, $m$ is the mantissa represented in fixed point, and $e$ is the exponent, also represented in fixed point.  Besides speed, a fixed point representation also offers imporvement in area and power.  An 8-bit fixed point multiply consumes $15.5 \times$ less energy and $12.4\times$ less area than a 32-bit fixed point multiply.  An 8-bit fixed point add consumes $30\times$ less energy and $116\times$ less area than a 32-bit floating point add. ~\cite{Horowitz}
