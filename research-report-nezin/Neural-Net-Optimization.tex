\subsection{Neural Net Optimization}
Deep neural networks were not designed with hardware efficiency in mind.  In order to tackle the difficult problem of both fitting a neural net on an FPGA and having it run fast, we should look at both optimization of the hardware itself and optimization of the neural net structure.  Existing approaches to optimizing neural networks for hardware fall into one of two categories: precision reduction and operation reduction. ~\cite{DBLP:journals/corr/SzeCYE17}  Precision reduction consists of reducing the number of bits used in computation, using fixed point arithmetic (GPUs use floating point), non-linear quantization, weight sharing, and Huffman coding. ~\cite{DBLP:journals/corr/HanMD15} Recent research has even taken this to extreme, constraining all weights to $\pm 1$. ~\cite{DBLP:journals/corr/CourbariauxB16}  Since custom hardware was not being seriously considered for neural networks until relatively recently, many of the algorithms for precision reduction are recently discovered.  However, operation reduction is capable of significantly reducing size and increasing speed even on GPUs and CPUs and therefore many powerfull ``pruning'' algorithms already exist such Optimal Brain Damage (OBD) ~\cite{Cun:1990:OBD:109230.109298} and Optimal Brain Surgery. ~\cite{Hassibi:1993:OBS:2987189.2987223}

\subsubsection{Quantization}
Some FPGAs have specialized hardware for fast multiplication.  In the case of Xilinx FPGAs, this fast multiplication hardware is only available for fixed point numbers, and a floating point implementation of a MAC would require large amount of hardware and would run slower.  Thus at a minimum requirement for functionality, the neural nets must be quantized to fixed point, or linear precision.  Obviously any amount of quantization introduces error, as an interesting aside, the signal to quantization error is given by roughly $6 dB$ per bit which can be derived from assuming that quantization error is uniformly distributed.  While it is fairly clear what this figure is in terms of quantizing the signal, it is not clear what exactly this implies for the quantization of filter coefficients.  When optimizing our structure, we are not limited to fixed schemes for weight quantization.  In particular, we know the exact weights we are using on every iteration and we can take advantage of that.  This has led to research being primarily focused on optimal weight quantization rather than activation (signal) quantization. ~\cite{DBLP:journals/corr/SzeCYE17}  

Before talking about advanced quantization techniques we give a brief review of typical number representations.  Fixed point representation is a class of number representations where the precision does not change.  That is, the number of digits before and after the binary point are constant.  These number are often represented in digital logic in the $Q$ format (Q is used since, roughly speaking, fixed point is to floating point as $\mathcal{Q}$ is to $\mathcal{R}$).  There are two parameters to a $Q$ representation, the number of digits before ($n$) and after ($m$) the binary point.  Precisely, the binary sequence $\{b_i\}_{i=0}^{n+m-1}$ has the associated value
$$ B = - b_0\times 2^{n-1} + \sum_{i=1}^{n+m-1} b_i \times 2^{n-i-1}$$

for example, assuming a two's complement representation, $Q1.7$ would have a maximum value of $\frac{127}{128}$, a minimum value of $-1$, and a precision of $\frac{1}{128}$.  The sequence $10010011$ would be associated with the value $-1 + \frac{1}{8} + \frac{1}{64} + \frac{1}{128} = -\frac{109}{128}$ 
An IEEE standard 32-bit floating point number is represented by 
$$ (-1)^s \times m \times 2^{(e-127)}$$
where $s$ is the sign bit, $m$ is the mantissa represented in fixed point, and $e$ is the exponent, also represented in fixed point.  Besides speed, a fixed point representation also offers imporvement in area and power.  An 8-bit fixed point multiply consumes $15.5 \times$ less energy and $12.4\times$ less area than a 32-bit fixed point multiply.  An 8-bit fixed point add consumes $30\times$ less energy and $116\times$ less area than a 32-bit floating point add. ~\cite{Horowitz}  Recent specialized deep learning platforms such as Google's TPU ~\cite{DBLP:journals/corr/JouppiYPPABBBBB17} and Nvidia's PASCAL ~\cite{PASCAL} have used 8-bit arithmetic.

Now we turn to non-linear quantization.  Since weights tend to be distributed non-uniformly (like a Gaussian or Laplacian distribution), it makes sense that the quantization should not be uniform in order to capture the maximum information.  One simple method is to use logarithmic quantization so that precision is more fine grained around zero where weights tend to cluster.  Some recent methods use weight sharing which means that some filter coefficients or weights in a fully connected layer are forced to be the same.  Obviously this has the advantage of reducing memory usage, but also leads to an increase in speed since the number of memory accesses is reduced.  In order to increase the effectiveness of this procedure, one may encourage clustering during training so that we end up with many sharable values. ~\cite{DBLP:journals/corr/HanMD15}

\subsubsection{Operation Reduction}
In the naive implementation of most neural networks, there are many redundant operations.  For instance a primary component of modern deep neural networks called the ``Rectified Linear Unit'' (ReLU) sets values less than 0 equal to 0.  This means that operations following will needlessly multiply by 0 repeatedly - if the corresponding multiply accumulates are skipped, throughput can be increased.  The zeros could also be more efficiently stored with a simple run length encoding to decrease memory usage. ~\cite{DBLP:journals/corr/SzeCYE17}

Regarding the pruning of neural networks, OBD and OBS are too difficult to apply to extremely deep networks because they are computationally expensive, and the cost scales with the number of weights.  A simpler method whereby weights with small values are eliminated was proposed in ~\cite{DBLP:journals/corr/HanPTD15} and proved to be very effective, reducing the size of deep neural nets by up to $80\%$.  There are also pruning methods which, instead of minimizing number of weights, attempt to minimize the energy usage of the neural net which is a strong function of DRAM access. ~\cite{DBLP:journals/corr/YangCS16a}  Note that all of these pruning methods introduce complications, storing arrays of values is simple because that is the natural structure of RAM and is easily indexed by two integers.  When the weights are sparse, we must come up with a scheme for efficiently storing only the nonzero values.  To combat this, structured pruning can be used which prunes entire groups of weights, resulting in a highly structured sparse matrix which is easy to work with. ~\cite{Yu:2017:SCD:3140659.3080215}

Instead of pruning after the fact, it is also possible to train neural networks in a constrained manner.  For instance, cascading two 3x3 filters can create a 5x5 filter.  Two 3x3 filters require 18 weights while a 5x5 would require 25.  Note that the same benefit cannot be drawn from cascading two 1-Dimensional filters, however this method is conceptually similar to a singular value decomposition wherein a matrix multiply can be approximated as the sum of lower rank matrices which do not require as many weights together.


